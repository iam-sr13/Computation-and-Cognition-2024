{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "1oBho425Q7ca"
      },
      "cell_type": "markdown",
      "source": [
        "# 1- Introduction to ANN\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://bernardmarr.com/img/What%20is%20an%20Artificial%20Neural%20Networks.jpg)"
      ],
      "metadata": {
        "id": "QKBKmwzDk_kX"
      }
    },
    {
      "metadata": {
        "id": "yLvtrsm0RcWq"
      },
      "cell_type": "markdown",
      "source": [
        "## Concept\n",
        "\n",
        "* The idea of an **Artificial Neural Network** (**ANN**) is to build a model based on the way the human brain **learns new** things.\n",
        "\n",
        "* It can be used in any type of **machine learning**. It learns by extracting the different underlying patterns in a given data.\n",
        "\n",
        "* This extraction is performed by stages, called **layers**. Each layer, which is composed by a set of **neurons**, will identify a certain pattern. The following layer, will identify another **more complex** pattern, from its previous layer.\n",
        "\n",
        "* The first layer, has the training data as input. It is called the** input layer**. In the last one, the output of the neurons are the final output.  It is called the **output** layer. The layers in between, are called **hidden layers**.\n",
        "* From now, the term **neural network** will mean **artificial neural network**."
      ]
    },
    {
      "metadata": {
        "id": "VL49qQMiRCfC"
      },
      "cell_type": "markdown",
      "source": [
        "# 2- Perceptron"
      ]
    },
    {
      "metadata": {
        "id": "QtI6YjSJRzo8"
      },
      "cell_type": "markdown",
      "source": [
        "## Definition\n",
        "\n",
        "* The term **Perceptron** refers to an input layer of data features values, with **forward weighted** connections to an output **layer ** of **one single neuron** , or of **multiple neurons**.\n",
        "\n",
        "* One of the simplest form of a neuron is an **LTU**.\n",
        "\n",
        "* **LTU**, for **L**inear **T**hreshold **U**nit, it is a component (**neuron**) that:\n",
        "  * Computes a **weighted sum** of its inputs: a linear function\n",
        "  * Applies a **step** function to the resulting sum, and **outputs** the results"
      ]
    },
    {
      "metadata": {
        "id": "Tg2exEipRz7w"
      },
      "cell_type": "markdown",
      "source": [
        "## The functions\n",
        "* The weighted sum function, is also called the **Propagation** function.\n",
        "* The step function, can be :\n",
        "  * A **non-linear** function, in this case, it will be  called the **threshold activation function** **(** This is the case of an LTU **)**. For example:\n",
        "    * Heaviside step function:\n",
        "    \n",
        "    $heaviside(z)=\\begin{cases}\n",
        "0,  & \\text{if $z < 0$} \\\\[2ex]\n",
        "1  & \\text{if $z >=0$}\n",
        "\\end{cases}\n",
        "$\n",
        "    * Sign function:\n",
        "        $sgn(z)=\\begin{cases}\n",
        "-1,  & \\text{if $z < 0$} \\\\[2ex]\n",
        "0  & \\text{if $z =0$} \\\\[2ex]\n",
        "1  & \\text{if $z >=0$}\n",
        "\\end{cases}\n",
        "$\n",
        "\n",
        "  * A **linear** function, simply called **activation function**. For example:\n",
        "    * The **identity function**: which means that the value computed by the propagation function, is the output value of the neuron.\n",
        "  * A **semi-linear** function, that is **monotonous** and **differentiable**. Also called **activation function**."
      ]
    },
    {
      "metadata": {
        "id": "lYnbKQUdR0Ou"
      },
      "cell_type": "markdown",
      "source": [
        "## Single Layer Perceptron\n",
        "* A **S**ingle **L**ayer **P**erceptron **(SLP)**, is simply a Perceptron with only one layer (**without** counting the **input layer** ) .\n",
        "* So, it is composed of an **input** layer and an **output** layer. The later one can have one ore more outputs. So, it can be used for binary and for multioutput classification\n",
        "\n",
        "* Considering an ANN in general, a **Perceptron** is considered as a **feedforward** neural network. We are going to talk about it in the next section.\n",
        "\n",
        "* An **SLP** an apply **2** different kind for **rules** to learn: **the perceptron** rule, or the **delta rule**. Each of the rules is associated with a certain type of activation function.  To apply the delta rule, we need the activation function to be differentiable."
      ]
    },
    {
      "metadata": {
        "id": "Nhm8OJc_RFU8"
      },
      "cell_type": "markdown",
      "source": [
        "# 3-Single Layer Perceptron"
      ]
    },
    {
      "metadata": {
        "id": "TOgMnI_CR7QE"
      },
      "cell_type": "markdown",
      "source": [
        "## SLP Learning\n",
        "<img src=\"https://docs.google.com/uc?export=download&id=1tL4Oi1SL7xjsPovWffuTpiyTh5pMhf6t\">\n"
      ]
    },
    {
      "metadata": {
        "id": "iJCObGCDR7Wk"
      },
      "cell_type": "markdown",
      "source": [
        "## SLP Learning: Perceptron Rule\n",
        "* To update the weights, the following formula is used:\n",
        "$w_{i,j}^{(next~step)} = w_{i,j} + \\eta \\cdot (  y_j -  \\hat y_j) \\cdot  x_i$\n",
        "* The concept is that each **wrong** prediction **reinforces** the **weight** corresponding to the **feature** that would contributed to the **correct prediction**. The computation of the weights is repeated until the samples are classified correctly."
      ]
    },
    {
      "metadata": {
        "id": "Zd06of0ZR7JC"
      },
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent\n",
        "\n",
        "* The concept of the** gradient** descent:\n",
        "  * With **initial parameters** of a model, predict an **output value**\n",
        "  * Compute the gradient of the “**error**” (“loss”) function (function of the **parameters** of the learning model) at a certain point= the slope of the surface of that function at that point calculated by its **derivative** at that point.\n",
        "  * Update the parameters in order to find the **local minima** by a step **proportional** to **the negative** of that **gradient**. (opposite direction ==> toward the local minima of the function). In the case of :\n",
        "    * **A stochastic gradient descent**: with **one sample**, **predict** →  **update** the parameters for the **next sample** → predict with the next sample with the new parameters\n",
        "    * A **Batch gradient descent**: predict for all samples **==1 epoch → update** the parameters** → **predict again with the new parameters\n",
        "  * Repeat the process in order to **minimize** the error."
      ]
    },
    {
      "metadata": {
        "id": "QrWP65T-VEVG"
      },
      "cell_type": "markdown",
      "source": [
        "## SLP Learning: Delta Rule\n",
        "\n",
        "\n",
        "* With the activation rule being **linear** or **semi-linear** but **differentiable**, the gradient descent is used to update the weights.\n",
        "* The weights are updated as follow:  $w_{i,j}^{next} = w_{i,j} + \\Delta w_{i,j}$\n",
        "  * In general:\n",
        " $\\Delta w = \\frac {- \\eta \\cdot \\partial E} {\\partial w}$\n",
        "  * In a case of **a linear activation function, and a Sum-Squared error function**\n",
        "    * In Online training:\n",
        "for a given sample:\n",
        "$\\Delta w_{ i,j } = \\eta\\cdot x_i \\cdot  ( y_j - \\hat y_j ) = \\eta \\cdot \\delta_j$\n",
        "    *  In Offline training:\n",
        "    $\\Delta w_{ i,j } = \\eta \\cdot   \\displaystyle \\sum_{s \\in X} x_i ^ s \\cdot  ( y_j ^s - \\hat y_j^s) = \\eta \\cdot   \\displaystyle \\sum_{s \\in X} x_i ^ s \\cdot \\delta_j ^{~s}$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Ca8Ofrs7RHwC"
      },
      "cell_type": "markdown",
      "source": [
        "# 4- SLP examples"
      ]
    },
    {
      "metadata": {
        "id": "xn8bZSmqSBwY"
      },
      "cell_type": "markdown",
      "source": [
        "## With sklearn: perceptron rule"
      ]
    },
    {
      "metadata": {
        "id": "2FiHnx64tsDM",
        "outputId": "121c25ba-1288-462b-c0f9-e83e52b4c7c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "#SLP using Perceptron class\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import Perceptron\n",
        "#load iris datasets\n",
        "X, y = load_iris(return_X_y=True)\n",
        "# instance of a perceptron: SLP with perceptron rule\n",
        "# learning rate =0.1, maximum of epoch = 5000, without shuffle after\n",
        "# each iteration\n",
        "mySLP = Perceptron(eta0=0.1, max_iter=5000,tol=1e-3,shuffle=False)\n",
        "mySLP.fit(X, y)\n",
        "print (\"The score of the classification (without shuffle) = \", mySLP.score(X, y) )\n",
        "\n",
        "# instance of an other perceptron: SLP with perceptron rule\n",
        "# learning rate =0.1, maximum of epoch = 20, with shuffle after\n",
        "# each iteration\n",
        "mySLPShuf = Perceptron(eta0=0.1, max_iter=20,tol=1e-3,shuffle=True)\n",
        "mySLPShuf.fit(X, y)\n",
        "print (\"The score of the classification (with shuffle) = \", mySLPShuf.score(X, y) )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The score of the classification (without shuffle) =  0.6666666666666666\n",
            "The score of the classification (with shuffle) =  0.48\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "rkBsxceXSnbA",
        "outputId": "35baf62b-a493-4a9b-bcf8-d2f45bff83d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "#SLP using SGDClassifier class\n",
        "mySGDCl = SGDClassifier(loss=\"perceptron\", learning_rate=\"constant\", eta0=0.1,penalty = None,\n",
        "                        max_iter=20, shuffle= True,tol=1e-3, verbose=2,random_state=0\n",
        "                       )\n",
        "mySGDCl.fit(X, y)\n",
        "\n",
        "print (\"The score of the classification (with shuffle) = \", mySGDCl.score(X, y) )\n",
        "\n",
        "print(\"The number of classes = \", len(mySGDCl.classes_))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 0.98, NNZs: 4, Bias: 0.100000, T: 150, Avg. loss: 0.084533\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1.15, NNZs: 4, Bias: 0.100000, T: 300, Avg. loss: 0.014240\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1.15, NNZs: 4, Bias: 0.100000, T: 450, Avg. loss: 0.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1.15, NNZs: 4, Bias: 0.100000, T: 600, Avg. loss: 0.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1.15, NNZs: 4, Bias: 0.100000, T: 750, Avg. loss: 0.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1.15, NNZs: 4, Bias: 0.100000, T: 900, Avg. loss: 0.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1.15, NNZs: 4, Bias: 0.100000, T: 1050, Avg. loss: 0.000000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1.15, NNZs: 4, Bias: 0.100000, T: 1200, Avg. loss: 0.000000\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 8 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 2.23, NNZs: 4, Bias: -0.100000, T: 150, Avg. loss: 1.168033\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1.30, NNZs: 4, Bias: 0.400000, T: 300, Avg. loss: 1.575120\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1.69, NNZs: 4, Bias: 0.600000, T: 450, Avg. loss: 1.338940\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 2.56, NNZs: 4, Bias: 0.700000, T: 600, Avg. loss: 1.161207\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 2.66, NNZs: 4, Bias: 0.800000, T: 750, Avg. loss: 1.082827\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 3.10, NNZs: 4, Bias: 0.900000, T: 900, Avg. loss: 1.121020\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 3.55, NNZs: 4, Bias: 1.100000, T: 1050, Avg. loss: 1.288540\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 4.03, NNZs: 4, Bias: 1.100000, T: 1200, Avg. loss: 1.366553\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 4.28, NNZs: 4, Bias: 1.300000, T: 1350, Avg. loss: 1.147813\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 4.26, NNZs: 4, Bias: 1.600000, T: 1500, Avg. loss: 1.206540\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 10 epochs took 0.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 1.25, NNZs: 4, Bias: -0.200000, T: 150, Avg. loss: 0.347400\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 2.26, NNZs: 4, Bias: -0.400000, T: 300, Avg. loss: 0.475360\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 3.19, NNZs: 4, Bias: -0.700000, T: 450, Avg. loss: 0.646093\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 4.38, NNZs: 4, Bias: -0.800000, T: 600, Avg. loss: 0.537720\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 5.13, NNZs: 4, Bias: -1.100000, T: 750, Avg. loss: 0.367080\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 5.81, NNZs: 4, Bias: -1.300000, T: 900, Avg. loss: 0.337140\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 6.21, NNZs: 4, Bias: -1.300000, T: 1050, Avg. loss: 0.270547\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 6.50, NNZs: 4, Bias: -1.500000, T: 1200, Avg. loss: 0.214500\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 6.84, NNZs: 4, Bias: -1.600000, T: 1350, Avg. loss: 0.250060\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 7.29, NNZs: 4, Bias: -1.800000, T: 1500, Avg. loss: 0.311633\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 7.64, NNZs: 4, Bias: -1.900000, T: 1650, Avg. loss: 0.288533\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 7.83, NNZs: 4, Bias: -1.900000, T: 1800, Avg. loss: 0.106353\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 7.96, NNZs: 4, Bias: -2.000000, T: 1950, Avg. loss: 0.151147\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 8.26, NNZs: 4, Bias: -2.000000, T: 2100, Avg. loss: 0.144180\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 8.60, NNZs: 4, Bias: -2.100000, T: 2250, Avg. loss: 0.295967\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 8.74, NNZs: 4, Bias: -2.200000, T: 2400, Avg. loss: 0.129960\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 8.94, NNZs: 4, Bias: -2.200000, T: 2550, Avg. loss: 0.120560\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 17 epochs took 0.04 seconds\n",
            "The score of the classification (with shuffle) =  0.48\n",
            "The number of classes =  3\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "5aXVjrUrRNGi"
      },
      "cell_type": "markdown",
      "source": [
        "# 5-Multi-Layer Perceptron\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_RT-oRvhVeFM"
      },
      "cell_type": "markdown",
      "source": [
        "## Multi-Layer Perceptron\n",
        "* An **MLP** (**M**ulti-**L**ayer **P**erceptron) is an **Perceptron** with **one** or **more** hidden layers.\n",
        "* It is another **Feed Forwad Artificial neural network**. Each of the layers (**except** the **output** layer) includes a **bias** neuron.\n",
        "* An **ANN** with more than **one hidden** layer is a **Deep Neural Network** ( **DNN** ).\n"
      ]
    },
    {
      "metadata": {
        "id": "yMgLDZAdSGjG"
      },
      "cell_type": "markdown",
      "source": [
        "## Backpropagation\n",
        "\n",
        "* It is a generalization of the **delta rule**. After a **forward pass**, a **backward** pass is applied to **update** the **weights** to back propagate the **errors**, using **gradient descent** procedure.\n",
        "* This forward/backward passes are repeated until the error function is minimized\n",
        "* The formula (of the generalized delta rule) is:\n",
        "  * $\\Delta_{w_{k,h}}= \\eta\\cdot o_k \\cdot \\delta_h$\n",
        "  * $    \\delta_h =\\begin{cases}\n",
        "\\acute f_{act} (net_h) \\cdot (y_h - \\hat y_h  )\\text{ ( h is an output neuron )}  \\\\[2ex]\n",
        "\\displaystyle \\acute f_{act} ( net_h ) \\cdot \\sum_{ l \\in L } \\delta_{ w_{h,l} } (\\text{ h is a neuron of a hidden layer })\n",
        "\\end{cases}\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)![](http://neuralnetworksanddeeplearning.com/images/tikz21.png)"
      ],
      "metadata": {
        "id": "XAJ1a7-DvEux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: http://neuralnetworksanddeeplearning.com/chap2.html"
      ],
      "metadata": {
        "id": "FIaBCw2Nv3b3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_classification(n_samples=100, random_state=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
        "                                                    random_state=1)\n",
        "clf = MLPClassifier((100, 20, 10), random_state=1, max_iter=300, verbose=2).fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2jyltXvn9HM",
        "outputId": "8c322fc7-c91e-43ac-84c8-f553980f5d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.81545675\n",
            "Iteration 2, loss = 0.79226787\n",
            "Iteration 3, loss = 0.76992007\n",
            "Iteration 4, loss = 0.74850131\n",
            "Iteration 5, loss = 0.72821812\n",
            "Iteration 6, loss = 0.70863050\n",
            "Iteration 7, loss = 0.68989699\n",
            "Iteration 8, loss = 0.67255262\n",
            "Iteration 9, loss = 0.65634602\n",
            "Iteration 10, loss = 0.64075428\n",
            "Iteration 11, loss = 0.62568604\n",
            "Iteration 12, loss = 0.61114003\n",
            "Iteration 13, loss = 0.59692429\n",
            "Iteration 14, loss = 0.58307344\n",
            "Iteration 15, loss = 0.56959320\n",
            "Iteration 16, loss = 0.55649240\n",
            "Iteration 17, loss = 0.54364814\n",
            "Iteration 18, loss = 0.53101550\n",
            "Iteration 19, loss = 0.51849106\n",
            "Iteration 20, loss = 0.50613941\n",
            "Iteration 21, loss = 0.49400706\n",
            "Iteration 22, loss = 0.48209372\n",
            "Iteration 23, loss = 0.47026554\n",
            "Iteration 24, loss = 0.45848628\n",
            "Iteration 25, loss = 0.44679954\n",
            "Iteration 26, loss = 0.43532251\n",
            "Iteration 27, loss = 0.42398921\n",
            "Iteration 28, loss = 0.41271617\n",
            "Iteration 29, loss = 0.40149812\n",
            "Iteration 30, loss = 0.39035000\n",
            "Iteration 31, loss = 0.37924024\n",
            "Iteration 32, loss = 0.36826581\n",
            "Iteration 33, loss = 0.35743098\n",
            "Iteration 34, loss = 0.34665372\n",
            "Iteration 35, loss = 0.33596315\n",
            "Iteration 36, loss = 0.32535482\n",
            "Iteration 37, loss = 0.31485862\n",
            "Iteration 38, loss = 0.30451938\n",
            "Iteration 39, loss = 0.29433966\n",
            "Iteration 40, loss = 0.28434061\n",
            "Iteration 41, loss = 0.27451934\n",
            "Iteration 42, loss = 0.26493184\n",
            "Iteration 43, loss = 0.25552890\n",
            "Iteration 44, loss = 0.24628817\n",
            "Iteration 45, loss = 0.23723715\n",
            "Iteration 46, loss = 0.22839723\n",
            "Iteration 47, loss = 0.21972353\n",
            "Iteration 48, loss = 0.21128307\n",
            "Iteration 49, loss = 0.20303091\n",
            "Iteration 50, loss = 0.19499043\n",
            "Iteration 51, loss = 0.18714838\n",
            "Iteration 52, loss = 0.17950312\n",
            "Iteration 53, loss = 0.17208374\n",
            "Iteration 54, loss = 0.16487636\n",
            "Iteration 55, loss = 0.15790961\n",
            "Iteration 56, loss = 0.15120196\n",
            "Iteration 57, loss = 0.14473690\n",
            "Iteration 58, loss = 0.13849539\n",
            "Iteration 59, loss = 0.13249620\n",
            "Iteration 60, loss = 0.12674917\n",
            "Iteration 61, loss = 0.12122453\n",
            "Iteration 62, loss = 0.11594395\n",
            "Iteration 63, loss = 0.11089008\n",
            "Iteration 64, loss = 0.10608583\n",
            "Iteration 65, loss = 0.10147143\n",
            "Iteration 66, loss = 0.09706998\n",
            "Iteration 67, loss = 0.09288316\n",
            "Iteration 68, loss = 0.08889826\n",
            "Iteration 69, loss = 0.08511982\n",
            "Iteration 70, loss = 0.08150263\n",
            "Iteration 71, loss = 0.07802562\n",
            "Iteration 72, loss = 0.07469679\n",
            "Iteration 73, loss = 0.07151739\n",
            "Iteration 74, loss = 0.06850726\n",
            "Iteration 75, loss = 0.06563954\n",
            "Iteration 76, loss = 0.06291087\n",
            "Iteration 77, loss = 0.06032002\n",
            "Iteration 78, loss = 0.05784457\n",
            "Iteration 79, loss = 0.05548261\n",
            "Iteration 80, loss = 0.05322843\n",
            "Iteration 81, loss = 0.05107897\n",
            "Iteration 82, loss = 0.04903599\n",
            "Iteration 83, loss = 0.04708817\n",
            "Iteration 84, loss = 0.04523155\n",
            "Iteration 85, loss = 0.04346477\n",
            "Iteration 86, loss = 0.04179645\n",
            "Iteration 87, loss = 0.04020831\n",
            "Iteration 88, loss = 0.03869703\n",
            "Iteration 89, loss = 0.03726533\n",
            "Iteration 90, loss = 0.03590628\n",
            "Iteration 91, loss = 0.03461381\n",
            "Iteration 92, loss = 0.03338279\n",
            "Iteration 93, loss = 0.03220624\n",
            "Iteration 94, loss = 0.03108157\n",
            "Iteration 95, loss = 0.03000917\n",
            "Iteration 96, loss = 0.02898433\n",
            "Iteration 97, loss = 0.02800367\n",
            "Iteration 98, loss = 0.02706964\n",
            "Iteration 99, loss = 0.02617648\n",
            "Iteration 100, loss = 0.02532824\n",
            "Iteration 101, loss = 0.02452079\n",
            "Iteration 102, loss = 0.02375171\n",
            "Iteration 103, loss = 0.02301683\n",
            "Iteration 104, loss = 0.02231404\n",
            "Iteration 105, loss = 0.02164293\n",
            "Iteration 106, loss = 0.02100048\n",
            "Iteration 107, loss = 0.02038580\n",
            "Iteration 108, loss = 0.01979744\n",
            "Iteration 109, loss = 0.01923311\n",
            "Iteration 110, loss = 0.01868442\n",
            "Iteration 111, loss = 0.01815739\n",
            "Iteration 112, loss = 0.01765313\n",
            "Iteration 113, loss = 0.01716908\n",
            "Iteration 114, loss = 0.01670346\n",
            "Iteration 115, loss = 0.01625608\n",
            "Iteration 116, loss = 0.01582545\n",
            "Iteration 117, loss = 0.01541110\n",
            "Iteration 118, loss = 0.01501308\n",
            "Iteration 119, loss = 0.01463058\n",
            "Iteration 120, loss = 0.01426375\n",
            "Iteration 121, loss = 0.01391170\n",
            "Iteration 122, loss = 0.01357269\n",
            "Iteration 123, loss = 0.01324628\n",
            "Iteration 124, loss = 0.01293209\n",
            "Iteration 125, loss = 0.01262951\n",
            "Iteration 126, loss = 0.01233823\n",
            "Iteration 127, loss = 0.01205671\n",
            "Iteration 128, loss = 0.01178539\n",
            "Iteration 129, loss = 0.01152380\n",
            "Iteration 130, loss = 0.01127220\n",
            "Iteration 131, loss = 0.01102969\n",
            "Iteration 132, loss = 0.01079486\n",
            "Iteration 133, loss = 0.01056804\n",
            "Iteration 134, loss = 0.01034882\n",
            "Iteration 135, loss = 0.01013663\n",
            "Iteration 136, loss = 0.00993129\n",
            "Iteration 137, loss = 0.00973252\n",
            "Iteration 138, loss = 0.00954026\n",
            "Iteration 139, loss = 0.00935412\n",
            "Iteration 140, loss = 0.00917373\n",
            "Iteration 141, loss = 0.00899869\n",
            "Iteration 142, loss = 0.00882922\n",
            "Iteration 143, loss = 0.00866488\n",
            "Iteration 144, loss = 0.00850538\n",
            "Iteration 145, loss = 0.00835082\n",
            "Iteration 146, loss = 0.00820111\n",
            "Iteration 147, loss = 0.00805591\n",
            "Iteration 148, loss = 0.00791484\n",
            "Iteration 149, loss = 0.00777770\n",
            "Iteration 150, loss = 0.00764427\n",
            "Iteration 151, loss = 0.00751444\n",
            "Iteration 152, loss = 0.00738829\n",
            "Iteration 153, loss = 0.00726543\n",
            "Iteration 154, loss = 0.00714571\n",
            "Iteration 155, loss = 0.00702897\n",
            "Iteration 156, loss = 0.00691548\n",
            "Iteration 157, loss = 0.00680477\n",
            "Iteration 158, loss = 0.00669670\n",
            "Iteration 159, loss = 0.00659144\n",
            "Iteration 160, loss = 0.00648890\n",
            "Iteration 161, loss = 0.00638876\n",
            "Iteration 162, loss = 0.00629095\n",
            "Iteration 163, loss = 0.00619548\n",
            "Iteration 164, loss = 0.00610244\n",
            "Iteration 165, loss = 0.00601156\n",
            "Iteration 166, loss = 0.00592274\n",
            "Iteration 167, loss = 0.00583610\n",
            "Iteration 168, loss = 0.00575126\n",
            "Iteration 169, loss = 0.00566836\n",
            "Iteration 170, loss = 0.00558736\n",
            "Iteration 171, loss = 0.00550821\n",
            "Iteration 172, loss = 0.00543101\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02481947, 0.97518053]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.predict_proba(X_test[:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nXEBrArpQNy",
        "outputId": "403c50f6-3b21-450e-8dae-7a6e83f5b86e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02481947, 0.97518053]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.predict(X_test[:5, :])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pReyNjBpHn6",
        "outputId": "ddedf5af-da54-4a5a-91d3-fa8ffb042f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y13r4UnppK-D",
        "outputId": "28d1888f-b3f4-4bb2-af6f-b86742bd6719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.88"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Zz9gugLgnUio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dENTYz9WnUpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "psZ78izOnUuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lYq3MErOnU1c"
      }
    },
    {
      "metadata": {
        "id": "A8DvRRWlRdGi"
      },
      "cell_type": "markdown",
      "source": [
        "# **Optional** **Study**: **NeuroLab**\n",
        "\n",
        "* Neurolab is Neural Network library for Python. It supports several types of neural networks.\n",
        "\n",
        "* For installation, just type: ```!pip install neurolab```\n",
        "\n",
        "* Like other machine learning techniques, a neural network need to be trained. Can be tested. And will be used to predict results.\n",
        "\n",
        "* Here is an example of how to use neurolab to create a neural network, and how to perform the fore-mentioned tasks:"
      ]
    },
    {
      "metadata": {
        "id": "j45OjrHyWBvK",
        "outputId": "3d2de099-a33d-4cd7-e29c-74bcef9f4eab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "cell_type": "code",
      "source": [
        "# installation of neurolab\n",
        "!pip install neurolab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting neurolab\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/fd/47a9a39158b461b6b862d64c0ad7f679b08ed6d316744299f0db89066342/neurolab-0.3.5.tar.gz (645kB)\n",
            "\u001b[K    100% |████████████████████████████████| 655kB 21.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: neurolab\n",
            "  Building wheel for neurolab (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c6/8f/37/32ab1cf4d601dc0bc49d7241012a4292db4b343bebff5b68e6\n",
            "Successfully built neurolab\n",
            "Installing collected packages: neurolab\n",
            "Successfully installed neurolab-0.3.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qk9_t_tuZn9K",
        "outputId": "23eb32b1-cdc0-44e1-b84e-593a0f363e92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import neurolab as nl\n",
        "# Create data\n",
        "myInput = np.random.uniform(-0.5, 0.5, (10, 2))\n",
        "# the labels correspond to the sum of two values\n",
        "myLabels = (myInput[:, 0] + myInput[:, 1]).reshape(10, 1)\n",
        "# Create network with 2 inputs, 5 neurons in hidden layer and 1 in output layer\n",
        "myNN = nl.net.newff([[-0.5, 0.5], [-0.5, 0.5]], [5, 1])\n",
        "# Train process\n",
        "myErr = myNN.train(myInput,myLabels, show=15)\n",
        "\n",
        "# Test and prediction process\n",
        "pred= myNN.sim([[0.2, 0.1]])\n",
        "# the result should be 0.3\n",
        "testErr= np.abs(0.3-pred)\n",
        "print (\"Prediction=\",pred)\n",
        "print (\"Test error = \",testErr)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 15; Error: 0.010827781453884296;\n",
            "The goal of learning is reached\n",
            "Prediction= [[0.27888876]]\n",
            "Test error =  [[0.02111124]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JdmOezr3oEou",
        "outputId": "07b938a4-edfc-4daa-d3e9-a5793c499ef2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Training errors until converngence:\",myErr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training errors until converngence: [0.32999912867545295, 0.2486661444665599, 0.1292945419367547, 0.0657565980545697, 0.05055297062542579, 0.036675825916531894, 0.02110533590934114, 0.01961050932435214, 0.017889590335269934, 0.017030460795990566, 0.01633692456059261, 0.015204223587932907, 0.013680298794147413, 0.012248254413184468, 0.010827781453884296, 0.010075997772901696, 0.009250550249635933]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OjzIaOe1RdZ2"
      },
      "cell_type": "markdown",
      "source": [
        "## Neurolab example details\n",
        "\n",
        "\n",
        "* Data: **10** samples described by **2** features. The labels are the **sum** of the  **2** features. In fact, the NN tries to model the **sum** function for values ranging from **-0.5** to **0.5 **\n",
        "\n",
        "* After creating the data, the steps were:\n",
        "   * Create an instance of a neural network with specified number of layers and neurons (``` nl.net.newff ```)\n",
        "   * Train the neural network (``` myNN.train ```)\n",
        "   * Predict the output for the value [0.2,0.1] (``` myNN.sim ```)\n",
        "   * Compute the test error (the true label is known: 0.2+0.1)"
      ]
    },
    {
      "metadata": {
        "id": "157T7zEfSB_u"
      },
      "cell_type": "markdown",
      "source": [
        "## Example with neurolab: delta rule"
      ]
    },
    {
      "metadata": {
        "id": "95gkfWzCxk2y",
        "outputId": "27463658-a640-45ec-e99f-4a195cd53d4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# size of the sample data\n",
        "print(\"The number of samples= \",X.shape[0])\n",
        "print(\"The number of Features = \",X.shape[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of samples=  150\n",
            "The number of Features =  4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "khI1OBVShPEU"
      },
      "cell_type": "code",
      "source": [
        "f1_mi= X[:,0].min()\n",
        "f1_ma= X[:,0].max()\n",
        "f2_mi= X[:,1].min()\n",
        "f2_ma= X[:,1].max()\n",
        "f3_mi=X[:,2].min()\n",
        "f3_ma= X[:,2].max()\n",
        "f4_mi=X[:,3].min()\n",
        "f4_ma= X[:,3].max()\n",
        "# instantiate an SLP with: 4 input nuerons for th 4 features\n",
        "# with 3 output neurons : to represent the 3 classes\n",
        "#  The learning rule is the Delta rule, the activation function is the\n",
        "# SoftMax function\n",
        "mySLPDelta = nl.net.newp([[f1_mi,f1_ma],[f2_mi,f2_ma],[f3_mi,f3_ma],[f4_mi,f4_ma]],3,\n",
        "                         transf=nl.trans.SoftMax())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GUZwaQkhm-fa"
      },
      "cell_type": "code",
      "source": [
        "# separate the data into test and train samples\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JO0iUqgOspU2"
      },
      "cell_type": "code",
      "source": [
        "# we have to construct the target values (true labels)\n",
        "# we will convert each of the labels vectors into a 3 columns\n",
        "# vector\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.utils import shuffle\n",
        "myLB = LabelBinarizer()\n",
        "y3_train= myLB.fit_transform(y_train)\n",
        "y3_test= myLB.fit_transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Qsn4zHS2IL2",
        "outputId": "8529db9d-5c53-4842-d642-d9a8927509d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_train[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "1lxOVKuL2OVo",
        "outputId": "78448f77-b868-44b7-c9d7-eddd57bae6c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "y3_train[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0],\n",
              "       [0, 0, 1],\n",
              "       [1, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "l_dqS1TnrJ0C",
        "outputId": "5ec70447-a30e-495b-8771-7de9a3d43674",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "# train the SLP\n",
        "myErr2 = mySLPDelta.train(x_train,y3_train, epochs=1000, show=100, lr=0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100; Error: 3.8853985221501404;\n",
            "Epoch: 200; Error: 3.1950292675296437;\n",
            "Epoch: 300; Error: 2.982660485152986;\n",
            "Epoch: 400; Error: 2.8946913259236964;\n",
            "Epoch: 500; Error: 2.824441033524367;\n",
            "Epoch: 600; Error: 2.7453913597776767;\n",
            "Epoch: 700; Error: 2.6523132225136887;\n",
            "Epoch: 800; Error: 2.547410831400115;\n",
            "Epoch: 900; Error: 2.43713444328122;\n",
            "Epoch: 1000; Error: 2.3293524688849825;\n",
            "The maximum number of train epochs is reached\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CE-EevcLgVgM"
      },
      "cell_type": "code",
      "source": [
        "# define a prediction function\n",
        "def Predict(x, Net):\n",
        "  if np.ndim(x)==1:\n",
        "    x=[x]\n",
        "  res = Net.sim(x)\n",
        "  return np.argmax(res,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CzZYJxOahZeA",
        "outputId": "5bdfedc8-18cc-4d68-ee5a-b4f6005b46dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# compute the accuracy of the SLP predictions\n",
        "# for the training and testing data\n",
        "\n",
        "yPred = Predict(x_train,mySLPDelta)\n",
        "accuracy =np.count_nonzero(yPred== y_train) / y_train.shape[0]\n",
        "print (\"Accuracy of the training = \",np.round(accuracy,2))\n",
        "\n",
        "yP_test = Predict(x_test,mySLPDelta)\n",
        "accuracy2 =np.count_nonzero(yP_test== y_test) / y_test.shape[0]\n",
        "print (\"Accuracy of the testing = \",np.round(accuracy2,2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the training =  0.98\n",
            "Accuracy of the testing =  0.97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z7prBx0pSGbS"
      },
      "cell_type": "markdown",
      "source": [
        "## MLP Example\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "5jfrUOkx1I18"
      },
      "cell_type": "code",
      "source": [
        "# an MLP with an input layer with 4 neurons, a hidden layer with\n",
        "# 2 neurons and an output layer with 3 neurons\n",
        "myMLP = nl.net.newff([[f1_mi,f1_ma],[f2_mi,f2_ma],[f3_mi,f3_ma],[f4_mi,f4_ma]],[2, 3],\n",
        "                         [nl.trans.TanSig(), nl.trans.SoftMax()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TKIB9vd1zY6k",
        "outputId": "9aa6fd45-9246-4018-cbc2-1e1e6d0f35dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# train the MLP\n",
        "myErr2 = myMLP.train(x_train,y3_train, epochs=10000, show=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The goal of learning is reached\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FsC4nBQS1Lke",
        "outputId": "bff2ef2f-be22-4705-aa26-f45fdc2eb88b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# compute the accuracy of the MLP predictions\n",
        "# for the training and testing data\n",
        "\n",
        "yPred = Predict(x_train,myMLP)\n",
        "accuracy =np.count_nonzero(yPred == y_train) / y_train.shape[0]\n",
        "print (\"Accuracy of the training = \",np.round(accuracy,2))\n",
        "\n",
        "yP_test = Predict(x_test,myMLP)\n",
        "accuracy2 =np.count_nonzero(yP_test== y_test) / y_test.shape[0]\n",
        "print (\"Accuracy of the testing = \",np.round(accuracy2,2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the training =  1.0\n",
            "Accuracy of the testing =  0.93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CffbTqAERQKg"
      },
      "cell_type": "markdown",
      "source": [
        "# 6- ANN topologies\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "23ma5K3xSKrS"
      },
      "cell_type": "markdown",
      "source": [
        "## FeedForward Neural Networks\n",
        "\n",
        "* We have already seen **feedforward neural networks** (SLP and MLP):\n",
        "  * One input layer + n hidden layers + one output layer (n>=1)\n",
        "  * Connection are only allowed to neurons of the following layers\n",
        "  * They can have shortcut connections: the connection are not set to the following layers but to subsequent layers"
      ]
    },
    {
      "metadata": {
        "id": "UtuV9VuAlvua",
        "outputId": "3a245819-1a73-414e-b379-8a41d93def4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install neupy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting neupy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/a2/4d8dc5d686adcdd3ed2c98c85ccf69370ef0675574b0f8dbb9b91abc978c/neupy-0.8.0-py2.py3-none-any.whl (224kB)\n",
            "\u001b[K    100% |████████████████████████████████| 225kB 7.5MB/s \n",
            "\u001b[?25hCollecting graphviz==0.5.1 (from neupy)\n",
            "  Downloading https://files.pythonhosted.org/packages/55/8d/18e45d3f57adfde20ac831a5ba7144b9e643185e05eb0a02b6f22d076752/graphviz-0.5.1-py2.py3-none-any.whl\n",
            "Collecting tensorflow<=1.12.0,>=1.10.1 (from neupy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cc/ca70b78087015d21c5f3f93694107f34ebccb3be9624385a911d4b52ecef/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl (83.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 83.1MB 318kB/s \n",
            "\u001b[?25hCollecting progressbar2==3.34.3 (from neupy)\n",
            "  Downloading https://files.pythonhosted.org/packages/87/31/b984e17bcc7491c1baeda3906fe3abc14cb5cd5dbd046ab46d9fc7a2edfd/progressbar2-3.34.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.6/dist-packages (from neupy) (3.0.2)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from neupy) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from neupy) (1.14.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from neupy) (2.8.0)\n",
            "Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=1.12.0,>=1.10.1->neupy) (1.12.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=1.12.0,>=1.10.1->neupy) (1.11.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=1.12.0,>=1.10.1->neupy) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=1.12.0,>=1.10.1->neupy) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=1.12.0,>=1.10.1->neupy) (1.0.9)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=1.12.0,>=1.10.1->neupy) (1.0.7)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=1.12.0,>=1.10.1->neupy) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=1.12.0,>=1.10.1->neupy) (0.32.3)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=1.12.0,>=1.10.1->neupy) (0.7.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=1.12.0,>=1.10.1->neupy) (0.7.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=1.12.0,>=1.10.1->neupy) (3.6.1)\n",
            "Requirement already satisfied: python-utils>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2==3.34.3->neupy) (2.3.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->neupy) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->neupy) (2.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->neupy) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->neupy) (2.5.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow<=1.12.0,>=1.10.1->neupy) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow<=1.12.0,>=1.10.1->neupy) (3.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<=1.12.0,>=1.10.1->neupy) (40.8.0)\n",
            "Installing collected packages: graphviz, tensorflow, progressbar2, neupy\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "  Found existing installation: tensorflow 1.13.0rc1\n",
            "    Uninstalling tensorflow-1.13.0rc1:\n",
            "      Successfully uninstalled tensorflow-1.13.0rc1\n",
            "  Found existing installation: progressbar2 3.38.0\n",
            "    Uninstalling progressbar2-3.38.0:\n",
            "      Successfully uninstalled progressbar2-3.38.0\n",
            "Successfully installed graphviz-0.5.1 neupy-0.8.0 progressbar2-3.34.3 tensorflow-1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-i_JoAEellhw",
        "outputId": "f5c7ce65-bd68-4201-9313-c9c20df503ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from neupy import plots\n",
        "\n",
        "# for visualization issues instead for setting the missing connection to 0\n",
        "# we set them to 0.01\n",
        "myFF = 2*(2*[0.01]+2*[3]+3*[0.01]) + 2* (4*[0.01]+ 3*[3]) + 3*(7*[0.01])\n",
        "myFF = np.array(myFF).reshape(7,7)\n",
        "#print(myFF)\n",
        "#hinton diagram corresponding to the feed forward neural network example\n",
        "plots.hinton(myFF,add_legend=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f65169f5630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATkAAAE5CAYAAADr4VfxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5IBoIgdEEzkgmsgB0f4Fs5NkFZpr8c0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "BLzCY4on4xFq",
        "outputId": "e3b84218-4edb-41ce-f273-78b1fa8ca6c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "# an other possible visualization\n",
        "# missing connections are represented\n",
        "# by black squares\n",
        "\n",
        "myFF2 = 2*(2*[-1]+2*[3]+3*[-1]) + 2* (4*[-1]+ 3*[3]) + 3*(7*[-1])\n",
        "myFF2 = np.array(myFF2).reshape(7,7)\n",
        "#print(myFF2)\n",
        "#hinton diagram corresponding to the feed forward neural network example\n",
        "plots.hinton(myFF2,add_legend=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6516c61a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATkAAAE5CAYAAADr4VfxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "WIGdB_nfSK9o"
      },
      "cell_type": "markdown",
      "source": [
        "## Recurrent Networks\n",
        "* Direct Recurrence:\n",
        "  * Multiple layers with connections allowed to neurons of the following layers\n",
        "  * A  neuron can also be connected to itself\n",
        "  \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "wlDH_OspBFrO",
        "outputId": "7295dfd8-3a5f-42a4-a87a-c61206a3c077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "# direct recurrent connections are represented by\n",
        "# smaller squares than the others connections\n",
        "myDR = ([1]+[0.01]+2*[3]+3*[0.01])+([0.01]+[1]+2*[3]+3*[0.01]) +  \\\n",
        "        (2*[0.01]+[1]+[0.01]+ 3*[3])+(3*[0.01]+[1]+ 3*[3]) +  \\\n",
        "        (4*[0.01] +[1]+ 2*[0.01])+ \\\n",
        "        (5*[0.01]+[1]+[0.01])+ \\\n",
        "        (6*[0.01]+[1])\n",
        "\n",
        "myDR = np.array(myDR).reshape(7,7)\n",
        "#print(myDR)\n",
        "plots.hinton(myDR,add_legend=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f65164f2d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATkAAAE5CAYAAADr4VfxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ynO5rKL6GhpI"
      },
      "cell_type": "markdown",
      "source": [
        "* Indirect Recurrence\n",
        "  * Multiple layers with connections allowed to neurons of the following layers\n",
        "  * Connections are also allowed between neurons and preceding layer\n"
      ]
    },
    {
      "metadata": {
        "id": "nUhHsDrkVTb2",
        "outputId": "91ab151b-fafe-4d2c-8bfb-2863b00e2866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "# indirect recurrent connections are represented by\n",
        "# smaller squares than the others connections\n",
        "myIR = 2*(2*[0.01]+2*[3]+3*[0.01])+  \\\n",
        "        2*(2*[1]+2*[0.01]+ 3*[3]) +  \\\n",
        "        3*(2*[0.01] + 2*[1]+3*[0.01])\n",
        "\n",
        "\n",
        "myIR = np.array(myIR).reshape(7,7)\n",
        "#print(myIR)\n",
        "plots.hinton(myIR,add_legend=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f651654ac88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATkAAAE5CAYAAADr4VfxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "qJf048USJpvm"
      },
      "cell_type": "markdown",
      "source": [
        "* Lateral  Recurrence\n",
        "  * Multiple layers with connections allowed to neurons of the following layers\n",
        "  * Connections between neurons of the same layer are allowed"
      ]
    },
    {
      "metadata": {
        "id": "I8kD9ULdniZw",
        "outputId": "e2879606-63ef-4afc-9f4d-c444dad2bcd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "# Lateral recurrent connections are represented by\n",
        "# smaller squares than the others connections\n",
        "myLR = ([0.01]+[1]+2*[3]+3*[0.01])+([1]+[0.01]+2*[3]+3*[0.01])+\\\n",
        "        (3*[0.01]+[1]+ 3*[3])+(2*[0.01]+[1]+[0.01]+ 3*[3])+\\\n",
        "        (5*[0.01] + 2*[1])+ \\\n",
        "        (4*[0.01]+[1]+[0.01]+[1])+ \\\n",
        "        (4*[0.01]+2*[1]+[0.01])\n",
        "\n",
        "myLR = np.array(myLR).reshape(7,7)\n",
        "#print(myLR)\n",
        "plots.hinton(myLR,add_legend=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6516223080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATkAAAE5CAYAAADr4VfxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "_6H6Ca-GSK4-"
      },
      "cell_type": "markdown",
      "source": [
        "## Fully connected Neural Network\n",
        "* Multiple layers with connections allowed from any neuron to any other neuron\n",
        "* Direct recurrence is not allowed\n",
        "* Connections must be symmetric"
      ]
    },
    {
      "metadata": {
        "id": "LFRL8cWVtCZU",
        "outputId": "6847be45-ed9f-4340-aedb-557188e192a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "myFC = 49 *[3]\n",
        "myFC = np.array(myFC).reshape(7,7)\n",
        "for i in range(7):\n",
        "  myFC[i,i] = 0.01\n",
        "#print(myFC)\n",
        "plots.hinton(myFC,add_legend=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6516064320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATkAAAE5CAYAAADr4VfxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkQOi/R9WayTWSYx4HAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "tSbN2yDrRSdW"
      },
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "* Joshi Prateek. Artificial intelligence with Python. Packt Publishing, 2017.\n",
        "\n",
        "* Jake VanderPlas. Python data science handbook: essential tools for working with data. O’Reilly Media, Inc, 2017.\n",
        "\n",
        "\n",
        "* Neural Networks – II, Version 2 CSE IIT, Kharagpur, available at:https://nptel.ac.in/courses/106105078/pdf/Lesson%2038.pdf\n",
        "Isaac Changhau, Loss Functions in Neural Networks, 2017, available at:\n",
        "https://isaacchanghau.github.io/post/loss_functions/\n",
        "\n",
        "* Sebastian Seung, The delta rule, MIT Department of Brain and Cognitive Sciences , Introduction to Neural Networks, 2005, https://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-641j-introduction-to-neural-networks-spring-2005/lecture-notes/lec19_delta.pdf\n",
        "* NeuPy, Neural Networks in Python, http://neupy.com/pages/home.html\n"
      ]
    }
  ]
}